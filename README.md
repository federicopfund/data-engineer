## Apliacion Spark 
 

### Descripción del proyecto:
>

 El objetivo principal del proyecto era construir una apliacion en apache spark que permita procesar archivos formato csv de ventas Onlines, Considerando un resultado de ventas Onlines se desea Normaliar en dimeniones esta tabla de echos proporcionada por el cliente.


## Responsabilidades clave y logros:

### Transformación ETL con Apache Spark:

>El diseño e implementación de procesos de Extracción, Transformación, Carga (ETL) utilizando tecnologías Apache Spark y Hadoop. Aseguró una transformación perfecta de datos internos y externos, estructurados y no estructurados.

### Almacenamiento de datos con Apache Hadoop:
>Implementé Apache Hadoop como solución de almacenamiento principal, optimizando los procesos de almacenamiento y recuperación de datos. Colaboré con el equipo para diseñar una arquitectura de almacenamiento de datos eficiente dentro del marco de Hadoop.

### Integración del entorno de Databricks:
>Utilicé el entorno Databricks para mejorar las capacidades colaborativas de ingeniería y ciencia de datos. Aseguré la integración fluida de Databricks en el ecosistema de BI existente para flujos de trabajo optimizados.

### Automatización de fábrica de datos de Azure:
>Implementé Azure Data Factory para automatizar la ejecución de la canalización ETL sin problemas. Diseñé y orquesté cuadernos "Pipeline", optimizando el flujo de trabajo general de procesamiento de datos.

### Integración de Data Lake:
>Integró con éxito la salida de los procesos ETL en un DataLake centralizado. Se garantizó la coherencia, accesibilidad y seguridad de los datos dentro del entorno de DataLake.

### Power BI Visualizacion:
>Aprovechó Power BI para visualizar y presentar datos transformados. Desarrollé paneles de control interactivos y reveladores para facilitar la toma de decisiones basada en datos.

### Resultado del proyecto:
>El proyecto dio como resultado una infraestructura de BI robusta y escalable, capaz de manejar diversos tipos de datos y requisitos de procesamiento. Al incorporar Apache Spark, Hadoop, Databricks, Azure Data Factory y Power BI, logramos una canalización de datos perfecta de un extremo a otro. Los procesos automatizados no solo mejoraron la eficiencia sino que también permitieron obtener información en tiempo real a través de visualizaciones dinámicas de Power BI.

### Conclusiones clave:
>Este proyecto mostró mi capacidad para navegar e integrar varias tecnologías de vanguardia en el panorama de BI. La ejecución exitosa del proyecto resalta mi competencia en el diseño, implementación y optimización de soluciones de BI de un extremo a otro que satisfacen las necesidades cambiantes de las organizaciones basadas en datos.
# 


**Testing and releasing**
<details>
<summary>Comandos</summary>
<br />

```
git tag -a v<0.0.3> -m "Release tag for version <0.0.3>"
git push origin --tags
```
<br />
</details>
<br />

**Testing and releasing**
<details>
<summary>v.0.0.1 </summary>
<br />
